# lambda/index.py - Updated to support Step Functions with Simple GitHub Scanning

import json
import boto3
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any
import os
from botocore.exceptions import ClientError
import re
import urllib3
import base64

# Set up logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# =============================================================================
# STEP 1: READ S3 DATA HANDLER
# =============================================================================
def read_s3_data_handler(event, context):
    """
    Step 1: Read IAM resource data from S3
    
    Handler: read_s3_data_handler
    Input: Step Function execution input
    Output: {
        "iam_data": {...},
        "users": [...],
        "metadata": {...}
    }
    """
    try:
        # Get environment variables
        bucket_name = os.environ.get('S3_BUCKET')
        s3_prefix = os.environ.get('S3_PREFIX', '')
        
        if not bucket_name:
            raise ValueError("S3_BUCKET environment variable not set")
        
        logger.info(f"Reading IAM data from bucket: {bucket_name}, prefix: {s3_prefix}")
        
        # Initialize S3 client
        s3_client = boto3.client('s3')
        
        # Construct S3 key
        s3_key = f"{s3_prefix}/latest.json" if s3_prefix else "latest.json"
        
        # Read the file
        response = s3_client.get_object(
            Bucket=bucket_name,
            Key=s3_key
        )
        
        content = response['Body'].read().decode('utf-8')
        iam_data = json.loads(content)
        
        # Extract user information
        users = []
        for user in iam_data['resources'].get('aws_iam_user', []):
            users.append({
                'name': user['name'],
                'arn': user['arn'],
                'tf_resource_name': user['tf_resource_name'],
                'source_file': user['source_file']
            })
        
        # Extract role information (for future use)
        roles = []
        for role in iam_data['resources'].get('aws_iam_role', []):
            roles.append({
                'name': role['name'],
                'arn': role['arn'],
                'tf_resource_name': role['tf_resource_name'],
                'source_file': role['source_file']
            })
        
        logger.info(f"Successfully parsed IAM data: {len(users)} users, {len(roles)} roles")
        
        # Return data for next step
        return {
            'statusCode': 200,
            'iam_data': iam_data,
            'users': users,
            'roles': roles,
            'metadata': {
                'timestamp': iam_data['metadata']['timestamp'],
                'account_id': iam_data['metadata']['account_id'],
                'total_resources': iam_data['metadata']['total_resources'],
                'users_count': len(users),
                'roles_count': len(roles)
            }
        }
        
    except Exception as e:
        logger.error(f"Error in read_s3_data_handler: {e}")
        return {
            'statusCode': 500,
            'error': f"Error reading S3 data: {str(e)}"
        }

# =============================================================================
# STEP 2: START CLOUDTRAIL QUERY HANDLER
# =============================================================================
def start_cloudtrail_query_handler(event, context):
    """
    Step 2: Start CloudTrail Lake query for user API usage
    """
    try:
        # Get input from previous step
        users = event.get('users', [])
        if not users:
            raise ValueError("No users provided from previous step")
        
        # Get environment variables
        event_data_store_arn = os.environ.get('CLOUDTRAIL_EVENT_DATA_STORE_ARN')
        retention_days = int(os.environ.get('CLOUDTRAIL_RETENTION_DAYS', '30'))
        
        if not event_data_store_arn:
            raise ValueError("CLOUDTRAIL_EVENT_DATA_STORE_ARN environment variable not set")
        
        logger.info(f"Starting CloudTrail Lake query for {len(users)} users, looking back {retention_days} days")
        
        # Initialize CloudTrail client
        cloudtrail_client = boto3.client('cloudtrail')
        
        # Calculate date range
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(days=retention_days)
        
        # Extract and clean user ARNs
        user_arns = []
        valid_users = []
        
        for user in users:
            arn = user.get('arn', '')
            # Skip ARNs with template variables
            if '${' not in arn and arn.startswith('arn:aws:iam:'):
                user_arns.append(arn)
                valid_users.append(user)
            else:
                logger.warning(f"Skipping user with templated ARN: {user.get('name', 'unknown')}")
        
        if not user_arns:
            logger.warning("No valid user ARNs found - all contain template variables")
            # Return a mock response for testing
            return {
                'statusCode': 200,
                'query_id': 'mock-query-no-valid-arns',
                'users': users,
                'query_details': {
                    'start_time': start_time.isoformat(),
                    'end_time': end_time.isoformat(),
                    'user_count': len(users),
                    'valid_user_count': 0,
                    'retention_days': retention_days,
                    'event_data_store_arn': event_data_store_arn,
                    'note': 'No valid ARNs found - using mock query ID'
                },
                'metadata': event.get('metadata', {}),
                'iam_data': event.get('iam_data', {}),
                'roles': event.get('roles', [])
            }
        
        # Build the CloudTrail Lake query (now passing the Event Data Store ARN)
        query = build_cloudtrail_query(user_arns, start_time, end_time, event_data_store_arn)
        
        logger.info(f"Executing CloudTrail Lake query from {start_time.isoformat()} to {end_time.isoformat()}")
        logger.info(f"Query: {query[:200]}...")  # Log first 200 chars of query for debugging
        
        # Start the query
        try:
            response = cloudtrail_client.start_query(
                QueryStatement=query
            )
            
            query_id = response['QueryId']
            logger.info(f"Started CloudTrail Lake query with ID: {query_id}")
            
        except ClientError as e:
            logger.error(f"CloudTrail start_query failed: {e}")
            # Return mock response for testing
            return {
                'statusCode': 200,
                'query_id': 'mock-query-failed-start',
                'users': users,
                'query_details': {
                    'start_time': start_time.isoformat(),
                    'end_time': end_time.isoformat(),
                    'user_count': len(users),
                    'valid_user_count': len(valid_users),
                    'retention_days': retention_days,
                    'event_data_store_arn': event_data_store_arn,
                    'error': str(e),
                    'note': 'CloudTrail query failed - using mock query ID'
                },
                'metadata': event.get('metadata', {}),
                'iam_data': event.get('iam_data', {}),
                'roles': event.get('roles', [])
            }
        
        return {
            'statusCode': 200,
            'query_id': query_id,
            'users': users,
            'query_details': {
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'user_count': len(users),
                'valid_user_count': len(valid_users),
                'retention_days': retention_days,
                'event_data_store_arn': event_data_store_arn
            },
            'metadata': event.get('metadata', {}),
            'iam_data': event.get('iam_data', {}),
            'roles': event.get('roles', [])
        }
        
    except Exception as e:
        logger.error(f"Error in start_cloudtrail_query_handler: {e}")
        return {
            'statusCode': 500,
            'error': f"CloudTrail error: {str(e)}"
        }

# =============================================================================
# STEP 3: CHECK CLOUDTRAIL QUERY STATUS AND GET RESULTS
# =============================================================================
def check_cloudtrail_query_handler(event, context):
    """
    Step 3: Check CloudTrail Lake query status and get results
    """
    try:
        # Get input from previous step
        query_id = event.get('query_id')
        users = event.get('users', [])
        
        if not query_id:
            raise ValueError("No query_id provided from previous step")
        
        logger.info(f"Checking status of CloudTrail query: {query_id}")
        
        # Initialize CloudTrail client
        cloudtrail_client = boto3.client('cloudtrail')
        
        # Check query status
        try:
            response = cloudtrail_client.describe_query(QueryId=query_id)
            query_status = response['QueryStatus']
            logger.info(f"Query status: {query_status}")
            
        except ClientError as e:
            logger.error(f"Failed to describe query: {e}")
            # Return finished status with empty results for now
            return {
                'statusCode': 200,
                'query_status': 'FINISHED',
                'query_id': query_id,
                'user_api_usage': {},
                'query_results_count': 0,
                'users': users,
                'metadata': event.get('metadata', {}),
                'iam_data': event.get('iam_data', {}),
                'roles': event.get('roles', []),
                'query_details': event.get('query_details', {}),
                'note': 'Query describe failed - using empty results'
            }
        
        if query_status == 'FINISHED':
            # Query completed successfully - get results
            logger.info("Query completed, retrieving results...")
            
            try:
                results_response = cloudtrail_client.get_query_results(QueryId=query_id)
                query_results = results_response.get('QueryResultRows', [])
                
                # Process the results
                user_api_usage = process_cloudtrail_results(query_results, users)
                
                logger.info(f"Processed {len(query_results)} query result rows into usage data for {len(user_api_usage)} users")
                
                return {
                    'statusCode': 200,
                    'query_status': 'FINISHED',
                    'query_id': query_id,
                    'user_api_usage': user_api_usage,
                    'query_results_count': len(query_results),
                    'users': users,
                    'metadata': event.get('metadata', {}),
                    'iam_data': event.get('iam_data', {}),
                    'roles': event.get('roles', []),
                    'query_details': event.get('query_details', {})
                }
                
            except ClientError as e:
                logger.error(f"Failed to get query results: {e}")
                # Return with empty results
                return {
                    'statusCode': 200,
                    'query_status': 'FINISHED',
                    'query_id': query_id,
                    'user_api_usage': {},
                    'query_results_count': 0,
                    'users': users,
                    'metadata': event.get('metadata', {}),
                    'iam_data': event.get('iam_data', {}),
                    'roles': event.get('roles', []),
                    'query_details': event.get('query_details', {}),
                    'note': 'Query results failed - using empty results'
                }
            
        elif query_status == 'FAILED':
            error_message = response.get('ErrorMessage', 'Unknown error')
            logger.error(f"CloudTrail query failed: {error_message}")
            return {
                'statusCode': 500,
                'query_status': 'FAILED',
                'error': f"CloudTrail query failed: {error_message}",
                'query_id': query_id
            }
            
        else:
            # Query still running (RUNNING, QUEUED)
            logger.info(f"Query still in progress: {query_status}")
            return {
                'statusCode': 202,
                'query_status': query_status,
                'query_id': query_id,
                'message': f"Query still {query_status.lower()}, need to wait longer",
                'users': users,
                'metadata': event.get('metadata', {}),
                'iam_data': event.get('iam_data', {}),
                'roles': event.get('roles', []),
                'query_details': event.get('query_details', {})
            }
        
    except Exception as e:
        logger.error(f"Error in check_cloudtrail_query_handler: {e}")
        return {
            'statusCode': 500,
            'query_status': 'FAILED',
            'error': f"Error checking CloudTrail query: {str(e)}"
        }

def process_cloudtrail_results(query_results: List[List], users: List[Dict[str, str]]) -> Dict[str, List[str]]:
    """
    Process CloudTrail Lake query results into user API usage
    """
    user_api_usage = {}
    
    # Create mapping from ARN to user name
    arn_to_name = {user['arn']: user['name'] for user in users}
    
    # Skip header row if present, process data rows
    data_rows = query_results[1:] if query_results and len(query_results) > 1 else query_results
    
    for row in data_rows:
        if len(row) < 3:  # Make sure we have minimum columns
            logger.warning(f"Skipping incomplete result row: {row}")
            continue
            
        try:
            user_arn = row[0]  # principal_arn
            event_name = row[1]
            event_source = row[2]
            
            # Only process users we're analyzing
            if user_arn in arn_to_name:
                user_name = arn_to_name[user_arn]
                
                if user_name not in user_api_usage:
                    user_api_usage[user_name] = []
                
                # Convert CloudTrail event to IAM action format
                iam_action = convert_event_to_iam_action(event_name, event_source)
                if iam_action and iam_action not in user_api_usage[user_name]:
                    user_api_usage[user_name].append(iam_action)
                    
        except (IndexError, ValueError) as e:
            logger.warning(f"Error processing result row {row}: {e}")
            continue
    
    return user_api_usage

def convert_event_to_iam_action(event_name: str, event_source: str) -> str:
    """Convert CloudTrail event to IAM action format"""
    # Remove .amazonaws.com suffix from event source
    service = event_source.replace('.amazonaws.com', '')
    
    # Simple mapping - can be expanded
    service_mappings = {
        's3': 's3',
        'iam': 'iam', 
        'ec2': 'ec2',
        'lambda': 'lambda',
        'rds': 'rds',
        'cloudformation': 'cloudformation',
        'sts': 'sts',
        'logs': 'logs',
        'cloudwatch': 'cloudwatch'
    }
    
    service_name = service_mappings.get(service, service)
    return f"{service_name}:{event_name}"

# =============================================================================
# STEP 4: FETCH TERRAFORM FILES FROM GITHUB - UPDATED WITH SIMPLE API CALLS
# =============================================================================
def fetch_terraform_files_handler(event, context):
    """
    Step 4: Fetch all .tf files from the GitHub repository using simple API calls
    
    Handler: fetch_terraform_files_handler
    Input: Output from Step 3 with user_api_usage
    Output: {
        "terraform_files": {...},
        "user_api_usage": {...},
        "users": [...],
        ...
    }
    """
    try:
        # Get input from previous steps
        user_api_usage = event.get('user_api_usage', {})
        users = event.get('users', [])
        
        logger.info(f"Fetching Terraform files for {len(users)} users")
        
        # Get environment variables
        github_repo = os.environ.get('GITHUB_REPO')
        github_token_ssm_path = os.environ.get('GITHUB_TOKEN_SSM_PATH')
        
        if not github_repo or not github_token_ssm_path:
            raise ValueError("GITHUB_REPO and GITHUB_TOKEN_SSM_PATH environment variables required")
        
        # Get GitHub token from SSM
        ssm_client = boto3.client('ssm')
        try:
            token_response = ssm_client.get_parameter(
                Name=github_token_ssm_path,
                WithDecryption=True
            )
            github_token = token_response['Parameter']['Value']
        except Exception as e:
            logger.error(f"Failed to get GitHub token: {e}")
            return create_error_response(event, f"Could not retrieve GitHub token: {str(e)}")
        
        # Fetch Terraform files using simple scanner
        try:
            terraform_files = fetch_terraform_files_from_github(github_repo, github_token)
            
            logger.info(f"Successfully fetched {len(terraform_files)} Terraform files")
            
            return {
                'statusCode': 200,
                'terraform_files': terraform_files,
                'terraform_files_count': len(terraform_files),
                'github_repo': github_repo,
                'scan_summary': {
                    'files_found': len(terraform_files),
                    'total_size_bytes': sum(len(content.encode('utf-8')) for content in terraform_files.values()),
                    'directories_scanned': len(set('/'.join(path.split('/')[:-1]) or 'root' for path in terraform_files.keys()))
                },
                # Pass through from previous steps
                'user_api_usage': user_api_usage,
                'users': users,
                'metadata': event.get('metadata', {}),
                'iam_data': event.get('iam_data', {}),
                'roles': event.get('roles', []),
                'query_details': event.get('query_details', {}),
                'query_results_count': event.get('query_results_count', 0)
            }
            
        except Exception as github_error:
            logger.error(f"GitHub API error: {github_error}")
            return create_error_response(event, f"Error fetching from GitHub: {str(github_error)}")
        
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return create_error_response(event, f"Unexpected error: {str(e)}")

def fetch_terraform_files_from_github(repo: str, token: str) -> Dict[str, str]:
    """
    UPDATED: Simple GitHub repository scanner using direct API calls - no PyGithub needed
    
    Args:
        repo: GitHub repository in format "owner/repo-name"
        token: GitHub personal access token
        
    Returns:
        Dict mapping file paths to file contents
    """
    logger.info(f"Scanning repository: {repo}")
    terraform_files = {}
    
    # Create HTTP pool manager
    http = urllib3.PoolManager()
    base_url = "https://api.github.com"
    
    def get_headers():
        """Get headers for GitHub API requests"""
        return {
            'Authorization': f'token {token}',
            'Accept': 'application/vnd.github.v3+json',
            'User-Agent': 'IAM-Analyzer/1.0'
        }
    
    def scan_directory(path: str = "", max_depth: int = 10, current_depth: int = 0):
        """Recursively scan directory for .tf files"""
        
        if current_depth > max_depth:
            logger.warning(f"Max depth reached for path: {path}")
            return
        
        try:
            # Get directory contents
            url = f"{base_url}/repos/{repo}/contents/{path}" if path else f"{base_url}/repos/{repo}/contents"
            
            response = http.request('GET', url, headers=get_headers())
            
            if response.status != 200:
                logger.error(f"GitHub API error for path {path}: {response.status}")
                return
            
            contents = json.loads(response.data.decode('utf-8'))
            
            # Handle single file response (convert to list)
            if isinstance(contents, dict):
                contents = [contents]
            
            for item in contents:
                item_path = item.get('path', '')
                item_type = item.get('type', '')
                item_name = item.get('name', '')
                
                if item_type == 'file' and item_name.endswith('.tf'):
                    # Get file content
                    file_content = get_file_content(item)
                    if file_content:
                        terraform_files[item_path] = file_content
                        logger.info(f"Retrieved: {item_path}")
                        
                elif item_type == 'dir':
                    # Skip common non-relevant directories
                    skip_dirs = {'.git', '.github', '.terraform', 'node_modules', '__pycache__', 
                               '.vscode', '.idea', 'dist', 'build'}
                    
                    if item_name not in skip_dirs:
                        scan_directory(item_path, max_depth, current_depth + 1)
                        
        except Exception as e:
            logger.error(f"Error scanning directory {path}: {e}")
    
    def get_file_content(file_item: dict):
        """Get file content from GitHub API response"""
        try:
            # Check if content is already included
            if 'content' in file_item:
                # Content is base64 encoded
                content = base64.b64decode(file_item['content']).decode('utf-8')
                return content
            
            # If not included, fetch it separately
            download_url = file_item.get('download_url')
            if download_url:
                response = http.request('GET', download_url)
                if response.status == 200:
                    return response.data.decode('utf-8')
            
            return None
            
        except Exception as e:
            logger.error(f"Error getting file content: {e}")
            return None
    
    try:
        # Start recursive scan from root
        scan_directory()
        
        logger.info(f"Found {len(terraform_files)} Terraform files")
        return terraform_files
        
    except Exception as e:
        logger.error(f"Error scanning repository: {e}")
        return {}

def create_error_response(event: dict, error_message: str) -> dict:
    """Create standardized error response that preserves data flow"""
    return {
        'statusCode': 500,
        'terraform_files': {},
        'terraform_files_count': 0,
        'error': error_message,
        'scan_summary': {
            'files_found': 0,
            'error': error_message
        },
        # PRESERVE all data from previous steps
        'user_api_usage': event.get('user_api_usage', {}),
        'users': event.get('users', []),
        'metadata': event.get('metadata', {}),
        'iam_data': event.get('iam_data', {}),
        'roles': event.get('roles', []),
        'query_details': event.get('query_details', {}),
        'query_results_count': event.get('query_results_count', 0)
    }

# =============================================================================
# STEP 5: PARSE TERRAFORM POLICIES  
# =============================================================================
def parse_terraform_policies_handler(event, context):
    """
    Step 5: Parse Terraform files to extract current IAM policies for users
    
    Handler: parse_terraform_policies_handler
    Input: Output from Step 4 with terraform_files
    Output: {
        "user_policies": {...},
        "policy_recommendations": {...},
        ...
    }
    """
    try:
        # Get input from previous steps
        terraform_files = event.get('terraform_files', {})
        user_api_usage = event.get('user_api_usage', {})
        users = event.get('users', [])
        
        if not terraform_files:
            logger.warning("No Terraform files provided from previous step")
            terraform_files = {}
        
        logger.info(f"Parsing {len(terraform_files)} Terraform files for {len(users)} users")
        
        # Parse current policies from Terraform files
        user_policies = parse_terraform_policies(terraform_files, users)
        
        # Generate recommendations
        policy_recommendations = generate_policy_recommendations(user_api_usage, user_policies)
        
        logger.info(f"Generated recommendations for {len(policy_recommendations)} users")
        
        return {
            'statusCode': 200,
            'user_policies': user_policies,
            'policy_recommendations': policy_recommendations,
            'recommendations_count': len(policy_recommendations),
            # Pass through from previous steps
            'terraform_files': terraform_files,
            'terraform_files_count': len(terraform_files),
            'user_api_usage': user_api_usage,
            'users': users,
            'metadata': event.get('metadata', {}),
            'iam_data': event.get('iam_data', {}),
            'roles': event.get('roles', []),
            'query_details': event.get('query_details', {}),
            'query_results_count': event.get('query_results_count', 0)
        }
        
    except Exception as e:
        logger.error(f"Error in parse_terraform_policies_handler: {e}")
        return {
            'statusCode': 500,
            'error': f"Error parsing Terraform policies: {str(e)}"
        }

def parse_terraform_policies(tf_files: Dict[str, str], users: List[Dict[str, str]]) -> Dict[str, Dict[str, Any]]:
    """Parse Terraform files to extract current IAM policies for users"""
    user_policies = {}
    
    # Create mapping from tf_resource_name to user name
    tf_name_to_user = {user['tf_resource_name']: user['name'] for user in users}
    
    for file_path, content in tf_files.items():
        logger.info(f"Parsing file: {file_path}")
        
        # Find inline user policies
        # Pattern: resource "aws_iam_user_policy" "name" { user = aws_iam_user.xyz.name ... }
        inline_policy_pattern = r'resource\s+"aws_iam_user_policy"\s+"([^"]+)"\s*\{([^{}]*(?:\{[^{}]*\}[^{}]*)*)\}'
        
        for match in re.finditer(inline_policy_pattern, content, re.DOTALL):
            policy_resource_name = match.group(1)
            policy_block = match.group(2)
            
            # Extract user reference
            user_match = re.search(r'user\s*=\s*aws_iam_user\.([^.\s]+)\.name', policy_block)
            if user_match:
                user_tf_name = user_match.group(1)
                
                if user_tf_name in tf_name_to_user:
                    user_name = tf_name_to_user[user_tf_name]
                    
                    if user_name not in user_policies:
                        user_policies[user_name] = {'inline_policies': [], 'attached_policies': []}
                    
                    # Extract policy document
                    policy_doc = extract_policy_document(policy_block)
                    
                    user_policies[user_name]['inline_policies'].append({
                        'name': policy_resource_name,
                        'file': file_path,
                        'policy_document': policy_doc
                    })
        
        # Find policy attachments
        # Pattern: resource "aws_iam_user_policy_attachment" "name" { user = aws_iam_user.xyz.name policy_arn = "..." }
        attachment_pattern = r'resource\s+"aws_iam_user_policy_attachment"\s+"([^"]+)"\s*\{([^{}]*(?:\{[^{}]*\}[^{}]*)*)\}'
        
        for match in re.finditer(attachment_pattern, content, re.DOTALL):
            attachment_resource_name = match.group(1)
            attachment_block = match.group(2)
            
            # Extract user reference
            user_match = re.search(r'user\s*=\s*aws_iam_user\.([^.\s]+)\.name', attachment_block)
            policy_match = re.search(r'policy_arn\s*=\s*"([^"]+)"', attachment_block)
            
            if user_match and policy_match:
                user_tf_name = user_match.group(1)
                policy_arn = policy_match.group(1)
                
                if user_tf_name in tf_name_to_user:
                    user_name = tf_name_to_user[user_tf_name]
                    
                    if user_name not in user_policies:
                        user_policies[user_name] = {'inline_policies': [], 'attached_policies': []}
                    
                    user_policies[user_name]['attached_policies'].append({
                        'name': attachment_resource_name,
                        'file': file_path,
                        'policy_arn': policy_arn
                    })
    
    logger.info(f"Parsed policies for {len(user_policies)} users")
    return user_policies

def extract_policy_document(policy_block: str) -> Dict[str, Any]:
    """Extract and parse policy document from Terraform block"""
    # Look for policy = jsonencode(...)
    policy_match = re.search(r'policy\s*=\s*jsonencode\s*\(\s*(\{.*?\})\s*\)', policy_block, re.DOTALL)
    
    if policy_match:
        policy_text = policy_match.group(1)
        try:
            # Basic parsing - convert Terraform syntax to JSON
            # This is simplified - a full parser would handle more edge cases
            policy_text = policy_text.replace('=', ':')
            policy_doc = json.loads(policy_text)
            return policy_doc
        except json.JSONDecodeError:
            logger.warning(f"Could not parse policy document: {policy_text[:100]}...")
            return {}
    
    return {}

def generate_policy_recommendations(user_api_usage: Dict[str, List[str]], 
                                  user_policies: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """Generate least privilege policy recommendations"""
    recommendations = {}
    
    for user_name in user_policies.keys():
        # Get current permissions (simplified)
        current_actions = set()
        
        # Extract actions from inline policies
        for policy in user_policies[user_name].get('inline_policies', []):
            policy_doc = policy.get('policy_document', {})
            statements = policy_doc.get('Statement', [])
            if not isinstance(statements, list):
                statements = [statements]
            
            for statement in statements:
                actions = statement.get('Action', [])
                if isinstance(actions, str):
                    actions = [actions]
                current_actions.update(actions)
        
        # Handle attached policies (simplified - would need to resolve ARNs)
        for policy in user_policies[user_name].get('attached_policies', []):
            if 'AdministratorAccess' in policy['policy_arn']:
                current_actions.add('*')
        
        # Get actual usage
        used_actions = set(user_api_usage.get(user_name, []))
        
        # Generate recommendations
        recommendations[user_name] = {
            'current_actions': list(current_actions),
            'used_actions': list(used_actions),
            'unused_actions': list(current_actions - used_actions),
            'missing_actions': list(used_actions - current_actions),
            'recommendation': 'remove_unused' if current_actions - used_actions else 'no_changes'
        }
        
        logger.info(f"User {user_name}: {len(current_actions)} current, {len(used_actions)} used, {len(current_actions - used_actions)} unused")
    
    return recommendations

# =============================================================================
# STEP 6: GENERATE GITHUB PR WITH RECOMMENDATIONS
# =============================================================================
def generate_github_pr_handler(event, context):
    """
    Step 6: Generate GitHub PR with policy recommendations
    
    Handler: generate_github_pr_handler
    Input: Output from Step 5 with policy_recommendations
    Output: {
        "pr_created": true/false,
        "pr_url": "...",
        "recommendations_applied": {...},
        ...
    }
    """
    try:
        # Get input from previous steps
        policy_recommendations = event.get('policy_recommendations', {})
        terraform_files = event.get('terraform_files', {})
        users = event.get('users', [])
        
        if not policy_recommendations:
            logger.info("No policy recommendations found - skipping PR generation")
            return create_pr_response(event, False, "No recommendations to apply")
        
        logger.info(f"Generating PR for {len(policy_recommendations)} user recommendations")
        
        # Get environment variables
        github_repo = os.environ.get('GITHUB_REPO')
        github_token_ssm_path = os.environ.get('GITHUB_TOKEN_SSM_PATH')
        
        if not github_repo or not github_token_ssm_path:
            raise ValueError("GITHUB_REPO and GITHUB_TOKEN_SSM_PATH environment variables required")
        
        # Get GitHub token from SSM
        ssm_client = boto3.client('ssm')
        try:
            token_response = ssm_client.get_parameter(
                Name=github_token_ssm_path,
                WithDecryption=True
            )
            github_token = token_response['Parameter']['Value']
        except Exception as e:
            logger.error(f"Failed to get GitHub token: {e}")
            return create_pr_response(event, False, f"Could not retrieve GitHub token: {str(e)}")
        
        # Generate PR with recommendations
        try:
            pr_generator = GitHubPRGenerator(github_repo, github_token)
            pr_result = pr_generator.create_recommendations_pr(
                policy_recommendations, 
                terraform_files,
                users
            )
            
            logger.info(f"PR generation result: {pr_result}")
            
            return {
                'statusCode': 200,
                'pr_created': pr_result['created'],
                'pr_url': pr_result.get('pr_url', ''),
                'pr_number': pr_result.get('pr_number', ''),
                'branch_name': pr_result.get('branch_name', ''),
                'files_modified': pr_result.get('files_modified', []),
                'recommendations_applied': len(policy_recommendations),
                'pr_summary': pr_result.get('summary', ''),
                # Pass through from previous steps
                'policy_recommendations': policy_recommendations,
                'terraform_files': terraform_files,
                'users': users,
                'metadata': event.get('metadata', {}),
                'iam_data': event.get('iam_data', {}),
                'user_api_usage': event.get('user_api_usage', {}),
                'query_details': event.get('query_details', {}),
                'query_results_count': event.get('query_results_count', 0)
            }
            
        except Exception as github_error:
            logger.error(f"GitHub PR generation error: {github_error}")
            return create_pr_response(event, False, f"Error creating PR: {str(github_error)}")
        
    except Exception as e:
        logger.error(f"Unexpected error in PR generation: {e}")
        return create_pr_response(event, False, f"Unexpected error: {str(e)}")

def create_pr_response(event: dict, created: bool, message: str) -> dict:
    """Create standardized PR response that preserves data flow"""
    return {
        'statusCode': 200 if created else 500,
        'pr_created': created,
        'pr_url': '',
        'pr_message': message,
        'recommendations_applied': len(event.get('policy_recommendations', {})),
        # PRESERVE all data from previous steps
        'policy_recommendations': event.get('policy_recommendations', {}),
        'terraform_files': event.get('terraform_files', {}),
        'users': event.get('users', []),
        'metadata': event.get('metadata', {}),
        'iam_data': event.get('iam_data', {}),
        'user_api_usage': event.get('user_api_usage', {}),
        'query_details': event.get('query_details', {}),
        'query_results_count': event.get('query_results_count', 0)
    }

class GitHubPRGenerator:
    """GitHub PR generator using direct API calls - no PyGithub needed"""
    
    def __init__(self, repo: str, token: str):
        self.repo = repo  # format: "owner/repo-name"
        self.token = token
        self.base_url = "https://api.github.com"
        self.http = urllib3.PoolManager()
        self.branch_name = f"iam-least-privilege-recommendations-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        
    def get_headers(self):
        """Get headers for GitHub API requests"""
        return {
            'Authorization': f'token {self.token}',
            'Accept': 'application/vnd.github.v3+json',
            'Content-Type': 'application/json',
            'User-Agent': 'IAM-Analyzer/1.0'
        }
    
    def create_recommendations_pr(self, recommendations: dict, terraform_files: dict, users: list) -> dict:
        """
        Create a GitHub PR with IAM policy recommendations
        
        Returns: {
            "created": bool,
            "pr_url": str,
            "pr_number": int,
            "branch_name": str,
            "files_modified": list,
            "summary": str
        }
        """
        try:
            logger.info(f"Creating PR for {len(recommendations)} recommendations")
            
            # Step 1: Get the default branch SHA
            main_branch_sha = self._get_main_branch_sha()
            if not main_branch_sha:
                raise Exception("Could not get main branch SHA")
            
            # Step 2: Create new branch
            branch_created = self._create_branch(main_branch_sha)
            if not branch_created:
                raise Exception("Could not create branch")
            
            # Step 3: Generate modified files
            modified_files = self._generate_modified_files(recommendations, terraform_files, users)
            
            # Step 4: Commit files to branch
            files_committed = self._commit_files_to_branch(modified_files)
            if not files_committed:
                raise Exception("Could not commit files")
            
            # Step 5: Create pull request
            pr_result = self._create_pull_request(recommendations, modified_files)
            
            logger.info(f"Successfully created PR: {pr_result.get('pr_url', 'unknown')}")
            
            return {
                'created': True,
                'pr_url': pr_result.get('pr_url', ''),
                'pr_number': pr_result.get('pr_number', ''),
                'branch_name': self.branch_name,
                'files_modified': list(modified_files.keys()),
                'summary': self._generate_pr_summary(recommendations)
            }
            
        except Exception as e:
            logger.error(f"Failed to create PR: {e}")
            return {
                'created': False,
                'error': str(e),
                'branch_name': self.branch_name,
                'files_modified': [],
                'summary': ''
            }
    
    def _get_main_branch_sha(self) -> str:
        """Get the SHA of the main/master branch"""
        try:
            # Try main first, then master
            for branch in ['main', 'master']:
                url = f"{self.base_url}/repos/{self.repo}/git/refs/heads/{branch}"
                response = self.http.request('GET', url, headers=self.get_headers())
                
                if response.status == 200:
                    data = json.loads(response.data.decode('utf-8'))
                    sha = data['object']['sha']
                    logger.info(f"Found {branch} branch SHA: {sha}")
                    return sha
            
            raise Exception("Could not find main or master branch")
            
        except Exception as e:
            logger.error(f"Error getting main branch SHA: {e}")
            return ""
    
    def _create_branch(self, base_sha: str) -> bool:
        """Create a new branch for the PR"""
        try:
            url = f"{self.base_url}/repos/{self.repo}/git/refs"
            
            data = {
                "ref": f"refs/heads/{self.branch_name}",
                "sha": base_sha
            }
            
            response = self.http.request(
                'POST', 
                url, 
                headers=self.get_headers(),
                body=json.dumps(data)
            )
            
            if response.status == 201:
                logger.info(f"Created branch: {self.branch_name}")
                return True
            else:
                logger.error(f"Failed to create branch: {response.status} {response.data}")
                return False
                
        except Exception as e:
            logger.error(f"Error creating branch: {e}")
            return False
    
    def _generate_modified_files(self, recommendations: dict, terraform_files: dict, users: list) -> dict:
        """Generate modified file contents with recommendations applied"""
        modified_files = {}
        
        # Group recommendations by file
        files_with_recommendations = {}
        for user_name, rec in recommendations.items():
            if rec['recommendation'] == 'remove_unused' and rec['unused_actions']:
                # Find which file contains this user's policies
                user_file = self._find_user_policy_file(user_name, terraform_files)
                if user_file:
                    if user_file not in files_with_recommendations:
                        files_with_recommendations[user_file] = []
                    files_with_recommendations[user_file].append((user_name, rec))
        
        # Modify each file
        for file_path, file_recommendations in files_with_recommendations.items():
            if file_path in terraform_files:
                original_content = terraform_files[file_path]
                modified_content = self._apply_recommendations_to_file(
                    original_content, 
                    file_recommendations, 
                    file_path
                )
                modified_files[file_path] = modified_content
        
        # Add analysis summary file
        summary_content = self._generate_analysis_summary(recommendations, users)
        modified_files['IAM_ANALYSIS_SUMMARY.md'] = summary_content
        
        logger.info(f"Generated {len(modified_files)} modified files")
        return modified_files
    
    def _find_user_policy_file(self, user_name: str, terraform_files: dict) -> str:
        """Find which file contains policies for a specific user"""
        for file_path, content in terraform_files.items():
            # Look for user policy references
            if f'user = aws_iam_user.{user_name.replace("-", "_")}.name' in content:
                return file_path
            if f'"{user_name}"' in content and 'aws_iam_user_policy' in content:
                return file_path
        return None
    
    def _apply_recommendations_to_file(self, content: str, file_recommendations: list, file_path: str) -> str:
        """Apply IAM recommendations to a Terraform file"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # Add header comment
        total_unused = sum(len(rec[1]['unused_actions']) for rec in file_recommendations)
        header = f"""# MODIFIED BY IAM ANALYZER - {timestamp}
# File: {file_path}
# Applied least privilege recommendations for {len(file_recommendations)} users
# Removed {total_unused} unused permissions

"""
        
        modified_content = content
        
        # Apply each recommendation
        for user_name, rec in file_recommendations:
            if rec['recommendation'] == 'remove_unused' and rec['unused_actions']:
                modified_content = self._remove_unused_permissions(
                    modified_content, 
                    user_name, 
                    rec
                )
        
        return header + modified_content
    
    def _remove_unused_permissions(self, content: str, user_name: str, rec: dict) -> str:
        """Remove unused permissions from a user's policy"""
        # This is a simplified implementation
        # In production, you'd want more sophisticated Terraform parsing
        
        user_tf_name = user_name.replace("-", "_")
        unused_count = len(rec['unused_actions'])
        
        # Add comment about the optimization
        optimization_comment = f"""
  # OPTIMIZED BY IAM ANALYZER - {datetime.now().strftime('%Y-%m-%d')}
  # Removed {unused_count} unused permissions: {', '.join(rec['unused_actions'][:5])}
  # Based on {30} days of CloudTrail analysis"""
        
        # For demo purposes, add the comment before user policies
        # In production, you'd actually modify the policy statements
        pattern = f'resource "aws_iam_user_policy.*{user_tf_name}'
        if re.search(pattern, content):
            modified = re.sub(
                f'(resource "aws_iam_user_policy[^{{]*{user_tf_name}[^{{]*{{)',
                f'\\1{optimization_comment}',
                content
            )
            return modified
        
        return content
    
    def _generate_analysis_summary(self, recommendations: dict, users: list) -> str:
        """Generate a markdown summary of the analysis"""
        total_users = len(users)
        users_with_unused = len([r for r in recommendations.values() if r['recommendation'] == 'remove_unused'])
        total_unused = sum(len(r['unused_actions']) for r in recommendations.values())
        
        summary = f"""# IAM Least Privilege Analysis Summary

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ðŸ“Š Analysis Results

- **Total users analyzed:** {total_users}
- **Users with unused permissions:** {users_with_unused}
- **Total unused permissions found:** {total_unused}
- **Potential security improvement:** {users_with_unused}/{total_users} users can be optimized

## ðŸŽ¯ Recommendations by User

"""
        
        for user_name, rec in recommendations.items():
            if rec['recommendation'] == 'remove_unused':
                summary += f"""### {user_name}
- **Current permissions:** {len(rec['current_actions'])} actions
- **Actually used:** {len(rec['used_actions'])} actions  
- **Unused permissions:** {len(rec['unused_actions'])} actions
- **Risk level:** {'ðŸ”´ HIGH' if '*' in rec['current_actions'] else 'ðŸŸ¡ MEDIUM' if len(rec['unused_actions']) > 10 else 'ðŸŸ¢ LOW'}

**Unused permissions to remove:**
```
{', '.join(rec['unused_actions'][:10])}
{'... and ' + str(len(rec['unused_actions']) - 10) + ' more' if len(rec['unused_actions']) > 10 else ''}
```

"""
        
        summary += f"""
## ðŸ” Analysis Details

- **Data source:** CloudTrail Lake (last 30 days)
- **Terraform files scanned:** All .tf files in repository
- **Analysis method:** Actual API usage vs. granted permissions

## âš¡ Next Steps

1. **Review the changes** in this PR carefully
2. **Test in a staging environment** before applying to production
3. **Monitor applications** after applying changes to ensure functionality
4. **Set up regular analysis** to maintain least privilege over time

## ðŸ›¡ï¸ Security Benefits

- **Reduced attack surface** - Fewer permissions mean less potential for abuse
- **Compliance improvement** - Better adherence to least privilege principle  
- **Easier auditing** - Cleaner, more focused IAM policies
- **Risk mitigation** - Limited blast radius if credentials are compromised

---
*Generated by IAM Least Privilege Analyzer*
"""
        
        return summary
    
    def _commit_files_to_branch(self, modified_files: dict) -> bool:
        """Commit modified files to the branch"""
        try:
            for file_path, content in modified_files.items():
                success = self._commit_single_file(file_path, content)
                if not success:
                    logger.error(f"Failed to commit {file_path}")
                    return False
                    
            logger.info(f"Successfully committed {len(modified_files)} files")
            return True
            
        except Exception as e:
            logger.error(f"Error committing files: {e}")
            return False
    
    def _commit_single_file(self, file_path: str, content: str) -> bool:
        """Commit a single file to the branch"""
        try:
            # Check if file exists first
            existing_file = self._get_file_sha(file_path)
            
            # Encode content to base64
            content_encoded = base64.b64encode(content.encode('utf-8')).decode('utf-8')
            
            url = f"{self.base_url}/repos/{self.repo}/contents/{file_path}"
            
            data = {
                "message": f"IAM Analyzer: Optimize {file_path} for least privilege",
                "content": content_encoded,
                "branch": self.branch_name
            }
            
            # If file exists, include its SHA for update
            if existing_file:
                data["sha"] = existing_file
            
            response = self.http.request(
                'PUT', 
                url, 
                headers=self.get_headers(),
                body=json.dumps(data)
            )
            
            if response.status in [200, 201]:
                logger.info(f"Committed {file_path}")
                return True
            else:
                logger.error(f"Failed to commit {file_path}: {response.status}")
                return False
                
        except Exception as e:
            logger.error(f"Error committing {file_path}: {e}")
            return False
    
    def _get_file_sha(self, file_path: str) -> str:
        """Get the SHA of an existing file, or None if it doesn't exist"""
        try:
            url = f"{self.base_url}/repos/{self.repo}/contents/{file_path}"
            response = self.http.request('GET', url, headers=self.get_headers())
            
            if response.status == 200:
                data = json.loads(response.data.decode('utf-8'))
                return data.get('sha', '')
            else:
                return ""
                
        except Exception:
            return ""
    
    def _create_pull_request(self, recommendations: dict, modified_files: dict) -> dict:
        """Create the pull request"""
        try:
            users_count = len([r for r in recommendations.values() if r['recommendation'] == 'remove_unused'])
            total_unused = sum(len(r['unused_actions']) for r in recommendations.values())
            
            title = f"ðŸ”’ IAM Least Privilege: Remove {total_unused} unused permissions across {users_count} users"
            
            body = f"""## ðŸ›¡ï¸ IAM Least Privilege Optimization

This PR automatically removes unused IAM permissions based on 30 days of CloudTrail analysis.

### ðŸ“Š Summary
- **Users optimized:** {users_count}
- **Unused permissions removed:** {total_unused}
- **Files modified:** {len(modified_files)}

### ðŸ” What Changed
"""
            
            for user_name, rec in recommendations.items():
                if rec['recommendation'] == 'remove_unused':
                    risk = 'ðŸ”´ HIGH RISK' if '*' in rec['current_actions'] else 'ðŸŸ¡ MEDIUM' if len(rec['unused_actions']) > 10 else 'ðŸŸ¢ LOW'
                    body += f"- **{user_name}** ({risk}): {len(rec['unused_actions'])} unused permissions\n"
            
            body += f"""
### ðŸ“ Files Modified
"""
            for file_path in modified_files.keys():
                body += f"- `{file_path}`\n"
            
            body += f"""
### âš ï¸ Important Notes
- **Test thoroughly** in staging before merging
- Changes are based on **last 30 days** of API usage
- **Monitor applications** after deployment
- See `IAM_ANALYSIS_SUMMARY.md` for detailed analysis

### ðŸŽ¯ Security Benefits
- âœ… Reduced attack surface
- âœ… Better compliance with least privilege
- âœ… Easier security auditing
- âœ… Limited blast radius if compromised

---
*ðŸ¤– Generated automatically by IAM Least Privilege Analyzer*
"""
            
            url = f"{self.base_url}/repos/{self.repo}/pulls"
            
            data = {
                "title": title,
                "body": body,
                "head": self.branch_name,
                "base": "main"  # or "master"
            }
            
            response = self.http.request(
                'POST', 
                url, 
                headers=self.get_headers(),
                body=json.dumps(data)
            )
            
            if response.status == 201:
                pr_data = json.loads(response.data.decode('utf-8'))
                logger.info(f"Created PR #{pr_data['number']}: {pr_data['html_url']}")
                
                return {
                    'pr_url': pr_data['html_url'],
                    'pr_number': pr_data['number']
                }
            else:
                logger.error(f"Failed to create PR: {response.status} {response.data}")
                return {}
                
        except Exception as e:
            logger.error(f"Error creating PR: {e}")
            return {}
    
    def _generate_pr_summary(self, recommendations: dict) -> str:
        """Generate a brief summary for logging"""
        users_count = len([r for r in recommendations.values() if r['recommendation'] == 'remove_unused'])
        total_unused = sum(len(r['unused_actions']) for r in recommendations.values())
        return f"Optimized {users_count} users, removed {total_unused} unused permissions"

def lambda_handler(event, context):
    """
    Lambda handler - routes to appropriate step handler
    """
    # Log the full event for debugging
    logger.info(f"Lambda received event keys: {list(event.keys())}")
    
    # Check if there's a Payload wrapper
    if 'Payload' in event:
        actual_event = event['Payload']
        step = event.get('step')
    else:
        actual_event = event
        step = event.get('step')
    
    logger.info(f"Extracted step: {step}")
    logger.info(f"Actual event keys: {list(actual_event.keys())}")
    
    if step == 'read_s3_data':
        logger.info("Routing to read_s3_data_handler")
        return read_s3_data_handler(actual_event, context)
    elif step == 'start_cloudtrail_query':
        logger.info("Routing to start_cloudtrail_query_handler")
        return start_cloudtrail_query_handler(actual_event, context)
    elif step == 'check_cloudtrail_query':
        logger.info("Routing to check_cloudtrail_query_handler")
        return check_cloudtrail_query_handler(actual_event, context)
    elif step == 'fetch_terraform_files':
        logger.info("Routing to fetch_terraform_files_handler")
        return fetch_terraform_files_handler(actual_event, context)
    elif step == 'parse_terraform_policies':
        logger.info("Routing to parse_terraform_policies_handler")
        return parse_terraform_policies_handler(actual_event, context)
    elif step == 'generate_github_pr':
        logger.info("Routing to generate_github_pr_handler")
        return generate_github_pr_handler(actual_event, context)
    else:
        logger.warning(f"No matching step found for: {step}")
        logger.info("Running default IAM analysis mode")
        return {
            'statusCode': 200,
            'message': 'IAM analysis completed',
            'debug_info': {
                'received_step': step,
                'available_steps': ['read_s3_data', 'start_cloudtrail_query', 'check_cloudtrail_query', 'fetch_terraform_files', 'parse_terraform_policies', 'generate_github_pr'],
                'event_keys': list(event.keys()),
                'actual_event_keys': list(actual_event.keys()) if 'actual_event' in locals() else 'N/A'
            }
        }

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================
def build_cloudtrail_query(user_arns: List[str], start_time: datetime, end_time: datetime, event_data_store_arn: str) -> str:
    """
    Build CloudTrail Lake SQL query to get user API usage
    """
    # Extract the Event Data Store ID from the ARN
    # ARN format: arn:aws:cloudtrail:region:account:eventdatastore/store-id
    store_id = event_data_store_arn.split('/')[-1]
    
    # Clean up ARNs - remove any template variables
    clean_arns = []
    for arn in user_arns:
        if '${' in arn:
            logger.warning(f"Skipping templated ARN: {arn}")
            continue
        clean_arns.append(arn)
    
    if not clean_arns:
        logger.warning("No valid ARNs found after filtering")
        # Return a query that will return no results but is valid
        return f"""
        SELECT 
            userIdentity.arn as principal_arn,
            eventName,
            eventSource,
            1 as call_count
        FROM {store_id}
        WHERE eventTime >= now() - interval '1' day
          AND userIdentity.arn = 'arn:aws:iam::000000000000:user/nonexistent'
        LIMIT 1
        """
    
    # Format ARNs for SQL IN clause
    arn_list = "', '".join(clean_arns)
    
    # Calculate retention period for the query
    retention_days = (end_time - start_time).days
    
    # Use your working query format
    query = f"""
    SELECT 
        userIdentity.arn as principal_arn,
        eventName,
        eventSource,
        COUNT(*) as call_count,
        MIN(eventTime) as first_seen,
        MAX(eventTime) as last_seen
    FROM {store_id}
    WHERE 
        eventTime >= now() - interval '{retention_days}' day
        AND userIdentity.arn IN ('{arn_list}')
        AND userIdentity.arn IS NOT NULL
        AND errorCode IS NULL
        AND eventName != 'AssumeRole'
        AND eventName != 'GetSessionToken'
        AND eventSource NOT LIKE '%signin.amazonaws.com'
    GROUP BY 
        userIdentity.arn, eventName, eventSource
    HAVING 
        COUNT(*) >= 1
    ORDER BY 
        principal_arn, call_count DESC
    LIMIT 1000
    """
    
    logger.info(f"Built CloudTrail query for {len(clean_arns)} valid ARNs using store ID: {store_id}")
    return query